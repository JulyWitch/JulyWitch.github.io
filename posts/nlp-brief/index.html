<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>NLP basics | Sajad Abdollahi</title>
<meta name=keywords content="nlp,natural-language-processing,machine-learning,cheatsheet"><meta name=description content="This guide provides an overview of text classification, exploring essential methods such as Naive Bayes, Logistic Regression, and Hidden Markov Models (HMMs). We’ll also cover critical concepts like n-gram models, embedding methods like TF-IDF and Word2Vec, and evaluation metrics including precision, recall, and F-measure. Whether you’re a beginner seeking to grasp the basics or an experienced practitioner looking to refine your skills, this guide will equip you with the knowledge to effectively tackle text classification tasks."><meta name=author content="Sajad"><link rel=canonical href=https://sajadabdollahi.ir/posts/nlp-brief/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://sajadabdollahi.ir/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://sajadabdollahi.ir/icons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://sajadabdollahi.ir/icons/favicon-32x32.png><link rel=apple-touch-icon href=https://sajadabdollahi.ir/icons/apple-icon-180x180.png><link rel=mask-icon href=https://sajadabdollahi.ir/icons/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://sajadabdollahi.ir/posts/nlp-brief/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="NLP basics"><meta property="og:description" content="This guide provides an overview of text classification, exploring essential methods such as Naive Bayes, Logistic Regression, and Hidden Markov Models (HMMs). We’ll also cover critical concepts like n-gram models, embedding methods like TF-IDF and Word2Vec, and evaluation metrics including precision, recall, and F-measure. Whether you’re a beginner seeking to grasp the basics or an experienced practitioner looking to refine your skills, this guide will equip you with the knowledge to effectively tackle text classification tasks."><meta property="og:type" content="article"><meta property="og:url" content="https://sajadabdollahi.ir/posts/nlp-brief/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-03-02T12:00:00-05:00"><meta property="article:modified_time" content="2017-03-02T12:00:00-05:00"><meta property="og:site_name" content="Sajad Abdollahi"><meta name=twitter:card content="summary"><meta name=twitter:title content="NLP basics"><meta name=twitter:description content="This guide provides an overview of text classification, exploring essential methods such as Naive Bayes, Logistic Regression, and Hidden Markov Models (HMMs). We’ll also cover critical concepts like n-gram models, embedding methods like TF-IDF and Word2Vec, and evaluation metrics including precision, recall, and F-measure. Whether you’re a beginner seeking to grasp the basics or an experienced practitioner looking to refine your skills, this guide will equip you with the knowledge to effectively tackle text classification tasks."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://sajadabdollahi.ir/posts/"},{"@type":"ListItem","position":2,"name":"NLP basics","item":"https://sajadabdollahi.ir/posts/nlp-brief/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"NLP basics","name":"NLP basics","description":"This guide provides an overview of text classification, exploring essential methods such as Naive Bayes, Logistic Regression, and Hidden Markov Models (HMMs). We’ll also cover critical concepts like n-gram models, embedding methods like TF-IDF and Word2Vec, and evaluation metrics including precision, recall, and F-measure. Whether you’re a beginner seeking to grasp the basics or an experienced practitioner looking to refine your skills, this guide will equip you with the knowledge to effectively tackle text classification tasks.\n","keywords":["nlp","natural-language-processing","machine-learning","cheatsheet"],"articleBody":"This guide provides an overview of text classification, exploring essential methods such as Naive Bayes, Logistic Regression, and Hidden Markov Models (HMMs). We’ll also cover critical concepts like n-gram models, embedding methods like TF-IDF and Word2Vec, and evaluation metrics including precision, recall, and F-measure. Whether you’re a beginner seeking to grasp the basics or an experienced practitioner looking to refine your skills, this guide will equip you with the knowledge to effectively tackle text classification tasks.\n1. Machine Learning Basics Supervised Learning Supervised learning is a type of machine learning where the model is trained on a labeled dataset. This means that for each example in the training set, the input data comes with a corresponding correct output (label). The model’s goal is to learn a mapping from inputs to outputs so that it can accurately predict the output for new, unseen data. Supervised learning is used in a wide range of applications, including image recognition, spam detection, and medical diagnosis. Common algorithms include linear regression, decision trees, and support vector machines.\nUnsupervised Learning Unsupervised learning involves training a model on data without labeled outputs. The model tries to learn the underlying structure of the data by identifying patterns, relationships, or groupings. Clustering and dimensionality reduction are two common tasks in unsupervised learning. For example, clustering algorithms like K-means can be used to group customers into segments based on purchasing behavior, while dimensionality reduction techniques like PCA (Principal Component Analysis) are used to simplify datasets by reducing the number of features.\nRegression Regression is a type of supervised learning where the goal is to predict a continuous output variable (also known as the dependent variable) based on one or more input variables (independent variables). The simplest form of regression is linear regression, where the relationship between the input and output is modeled as a straight line. Regression models are widely used in forecasting (e.g., predicting sales, stock prices) and determining the strength of relationships between variables.\nClassification Classification is another type of supervised learning where the goal is to assign input data to one of several predefined categories or classes. Unlike regression, where the output is continuous, the output in classification is discrete. For example, a spam filter classifies emails as either “spam” or “not spam.” Common algorithms for classification include logistic regression, decision trees, and support vector machines. In multi-class classification, the model predicts which one of several possible classes an instance belongs to.\n2. Optimization in Machine Learning Learning Rate (Alpha) The learning rate, often denoted by the Greek letter alpha (α), is a crucial hyperparameter in the training of machine learning models. It controls the size of the steps taken by the optimization algorithm (such as gradient descent) when adjusting the model’s weights. If the learning rate is too high, the model might overshoot the optimal solution, leading to divergence or instability. If the learning rate is too low, the training process can become very slow, and the model may get stuck in local minima. Finding the right learning rate is essential for efficient and effective training.\nGradient Descent Gradient Descent is an iterative optimization algorithm used to minimize the cost function in machine learning models. The cost function measures how well the model’s predictions match the actual data. Gradient Descent works by calculating the gradient (partial derivative) of the cost function concerning the model’s parameters (weights). The model parameters are then updated in the opposite direction of the gradient, hence the term “descent.” This process is repeated until the model converges to a minimum point in the cost function, ideally the global minimum.\nStochastic Gradient Descent (SGD) Stochastic Gradient Descent (SGD) is a variation of the gradient descent algorithm. Unlike traditional gradient descent, which computes the gradient using the entire dataset, SGD updates the model parameters after each training example. This makes SGD faster and more suitable for large datasets, but it introduces more noise into the training process, leading to more fluctuations in the cost function. However, this noise can help the model escape local minima and potentially find a better overall solution.\nBatch Gradient Descent Batch Gradient Descent is the traditional form of gradient descent, where the gradient of the cost function is computed using the entire dataset before updating the model’s parameters. This method provides a smooth convergence path, as the gradient calculation is based on the overall direction of the dataset. However, it can be computationally expensive and slow, especially for large datasets, as it requires loading the entire dataset into memory and performing the gradient calculation for each parameter update.\nMini-Batch Gradient Descent Mini-Batch Gradient Descent is a compromise between Stochastic Gradient Descent and Batch Gradient Descent. It splits the dataset into small, manageable batches and updates the model’s parameters for each batch. This method balances the benefits of both SGD and Batch Gradient Descent. It offers the efficiency and speed of SGD while reducing the noise in the updates, leading to a smoother convergence path. Mini-Batch Gradient Descent is widely used in practice, particularly in training neural networks.\n3. Regression Techniques Linear Regression Linear Regression is one of the simplest and most commonly used regression algorithms. It models the relationship between a dependent variable (output) and one or more independent variables (inputs) as a linear equation. The equation takes the form:\n[ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n ]\nwhere ( y ) is the predicted output, ( x_1, x_2, \\dots, x_n ) are the input variables, and ( \\beta_0, \\beta_1, \\dots, \\beta_n ) are the coefficients (weights) that the model learns during training. The goal of linear regression is to find the line (or hyperplane in higher dimensions) that best fits the data by minimizing the difference between the predicted values and the actual values. Linear regression is often used in forecasting and understanding the relationship between variables.\nLogistic Regression Logistic Regression is a classification algorithm used for binary classification tasks, where the output variable can take on two possible values (e.g., yes/no, 0/1, true/false). Instead of predicting a continuous output, logistic regression predicts the probability that an instance belongs to a particular class. It uses the logistic function (also known as the sigmoid function) to map the predicted values to probabilities between 0 and 1. The output is then classified based on a threshold, usually 0.5. Despite its name, logistic regression is primarily used for classification rather than regression.\nLogistic Regression for Multi-Class Problems While logistic regression is inherently a binary classifier, it can be extended to handle multi-class classification problems through techniques such as One-vs-Rest (OvR) or Softmax Regression.\nOne-vs-Rest (OvR): In this approach, multiple binary classifiers are trained, one for each class. Each classifier distinguishes one class from all the others, and the final prediction is based on the classifier with the highest confidence. Softmax Regression: This is an extension of logistic regression that directly handles multi-class classification. It uses the softmax function to predict the probabilities of each class, and the class with the highest probability is chosen as the prediction. 4. Regularization Regularization (A Solution to Overfitting) Regularization is a technique used to prevent overfitting, which occurs when a model learns the noise and details in the training data to the detriment of its performance on new data. Regularization works by adding a penalty term to the cost function, which discourages the model from becoming too complex. The most common types of regularization are:\nL1 Regularization (Lasso): Adds the absolute value of the coefficients as a penalty to the cost function. This can lead to sparse models where some feature weights are reduced to zero, effectively performing feature selection. L2 Regularization (Ridge): Adds the square of the coefficients as a penalty to the cost function. This tends to distribute the error among all features, reducing the impact of any one feature on the model’s predictions. Regularization Rate (Lambda) The regularization rate, denoted by lambda (λ), controls the strength of the penalty applied during regularization. A higher lambda increases the penalty, leading to a simpler model that may generalize better to new data but may underfit the training data. Conversely, a lower lambda reduces the penalty, allowing the model to capture more complex patterns in the training data, but increasing the risk of overfitting. Tuning the regularization rate is crucial for finding the right balance between bias and variance in the model.\n5. Neural Networks (NN) Neural Network A Neural Network is a computational model inspired by the structure and function of the human brain. It consists of layers of interconnected nodes (neurons), where each connection has an associated weight. Neural networks are capable of learning complex patterns and relationships in data through a process called training. They are particularly powerful in tasks like image recognition, natural language processing, and game playing.\nA basic neural network consists of three types of layers:\nInput Layer: The layer that receives the input data. Hidden Layers: Layers between the input and output layers where the network learns to represent the data. The more hidden layers, the deeper the network, allowing it to learn more complex features. Output Layer: The layer that produces the final output, such as class labels or predicted values. Non-Linear Classification Non-Linear Classification refers to the classification of data that cannot be separated by a straight line or linear boundary. In many real-world scenarios, the relationship between the features and the target variable is non-linear. Neural networks are well-suited for non-linear classification because they can learn complex, non-linear decision boundaries through multiple layers of neurons and non-linear activation functions.\nXNOR Neural Network An XNOR Neural Network is a simple example of a neural network that can solve the XNOR logic problem, which is a non-linear classification problem. The XNOR gate outputs true only when both inputs are the same (either both true or both false). A single-layer neural network cannot solve this problem, but a neural network with at least one hidden layer can, demonstrating the power of non-linear decision boundaries.\nNN for Classification Neural networks are commonly used for classification tasks, where the goal is to assign input data to one of several categories. In a classification neural network, the output layer typically uses a softmax activation function for multi-class classification, which converts the network’s outputs into probabilities. The class with the highest probability is chosen as the predicted category.\nNN for Regression Neural networks can also be used for regression tasks, where the goal is to predict a continuous output value. In a regression neural network, the output layer typically uses a linear activation function to produce continuous values. The network learns to map input features to a continuous output through training on labeled data.\nCost Function The cost function, also known as the loss function, measures the difference between the predicted output of the neural network and the actual output. The goal of training is to minimize this cost function, making the predictions as accurate as possible. Common cost functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\nBackpropagation Backpropagation is the algorithm used to train neural networks by computing the gradient of the cost function with respect to each weight in the network. It involves two phases:\nForward Pass: The input data is passed through the network to calculate the output and the cost. Backward Pass: The gradient of the cost function is calculated using the chain rule of calculus, and the weights are updated accordingly to minimize the cost. This process is repeated for multiple iterations (epochs) until the model converges. Neurons Count in Neural Network The number of neurons in each layer of a neural network determines the network’s capacity to learn from data. More neurons allow the network to capture more complex features, but too many neurons can lead to overfitting. The optimal number of neurons depends on the complexity of the task and the amount of training data available.\nLayers Count in Neural Network The number of layers in a neural network, particularly the number of hidden layers, determines the depth of the network. A deeper network with more layers can learn more abstract features and represent more complex relationships in the data. However, deeper networks are also more computationally expensive to train and can suffer from issues like vanishing gradients. The right number of layers depends on the specific problem and the data.\nSteps to Create a Neural Network Choose Neurons and Layers Count and Biases:\nSelect the number of neurons in each layer and the number of layers based on the problem’s complexity. Initialize weights and biases, usually with small random values. Training:\nFeed the training data through the network, performing forward and backward passes to update the weights. Apply Cost Function:\nCalculate the error between the predicted output and the actual output using the cost function. Apply Backpropagation:\nPerform backpropagation to calculate gradients and update weights. Adjust Weights:\nIterate through the training process multiple times (epochs) to adjust the weights and minimize the cost function. 6. Evaluating Model Performance Evaluate Hypothesis Evaluating a hypothesis in the context of machine learning involves assessing how well a model (or hypothesis) performs in making predictions on new, unseen data. The hypothesis refers to the model’s assumptions about the underlying data distribution and its ability to generalize from the training data to the test data. Evaluation metrics such as accuracy, precision, recall, F1-score, and the confusion matrix are commonly used to determine the effectiveness of the model. The goal is to ensure that the model’s predictions are accurate and reliable, minimizing errors on both the training and test sets.\nOverfitting Overfitting occurs when a machine learning model performs exceptionally well on the training data but fails to generalize to new, unseen data. This happens when the model learns not only the underlying patterns in the data but also the noise and outliers. As a result, the model becomes overly complex and performs poorly on the test set. Overfitting can be identified when there’s a large discrepancy between the training error (low) and the test error (high). Techniques like regularization, cross-validation, and simplifying the model can help prevent overfitting.\nTraining Error Training error is the error (or loss) calculated on the training dataset, which is the data the model was trained on. It measures how well the model fits the training data. A low training error indicates that the model has learned the patterns in the training data well. However, a very low training error, especially if the test error is high, might indicate that the model has overfitted the training data, capturing even the noise instead of just the underlying trends.\nCross-Validation Error Cross-validation error is the error calculated during the cross-validation process, where the training dataset is split into multiple subsets (folds), and the model is trained and validated on different combinations of these subsets. The most common technique is k-fold cross-validation, where the data is divided into k subsets, and the model is trained k times, each time leaving out one of the subsets for validation. The cross-validation error provides a more reliable estimate of the model’s performance on unseen data compared to just using the training error. It helps in tuning hyperparameters and selecting the best model.\nTest Error Test error is the error calculated on the test dataset, which is a separate portion of the data not used during the training process. The test error gives an unbiased estimate of how well the model is likely to perform on new, unseen data. A low test error indicates that the model generalizes well, while a high test error might indicate problems like overfitting or underfitting. The goal is to minimize the test error, ensuring that the model’s predictions are accurate on real-world data.\n7. Bias and Variance Tradeoff Underfitting: Bias Underfitting occurs when a model is too simple to capture the underlying patterns in the data, leading to poor performance on both the training and test sets. This usually happens when the model has high bias, meaning it makes strong assumptions about the data that aren’t accurate. High bias models tend to miss the relevant relationships between features and the target variable. Techniques to reduce underfitting include increasing the model complexity (e.g., adding more features, using a more complex model), or reducing regularization.\nOverfitting: Variance Overfitting, as mentioned earlier, happens when a model is too complex and captures the noise in the training data along with the underlying patterns. This leads to high variance, meaning the model is highly sensitive to the specific training data and may perform poorly on new data. High variance models have low training error but high test error. Techniques like cross-validation, regularization, and pruning (in decision trees) can help reduce overfitting and control variance.\n8. Regular Expressions (Regex) in NLP Regex A Regular Expression (Regex) is a sequence of characters that define a search pattern. In computer science, regex is used for pattern matching within strings. It is a powerful tool for text processing tasks like searching, extracting, and replacing text. For example, the regex pattern \\d+ matches one or more digits in a text. Regex is widely used in programming for tasks such as validating inputs, parsing text, and transforming data.\nRegex Usage in NLP In Natural Language Processing (NLP), regex is frequently used for text preprocessing tasks such as tokenization, removing unwanted characters (like punctuation or special symbols), and extracting specific patterns (like dates, emails, or phone numbers) from text. Regex can also be used to identify and extract linguistic patterns, such as matching specific word sequences or sentence structures. Its flexibility makes it a valuable tool for cleaning and preparing text data before applying more complex NLP algorithms.\nCommon Regexes Common regular expressions include:\n\\d+: Matches one or more digits. \\w+: Matches one or more word characters (letters, digits, and underscores). \\s+: Matches one or more whitespace characters. [A-Za-z]+: Matches one or more uppercase or lowercase letters. ^ and $: Match the start and end of a string, respectively. .: Matches any single character except a newline. These patterns can be combined and modified using quantifiers, character classes, and anchors to create complex expressions tailored to specific tasks.\n9. NLP Steps Tokenizing Tokenization is the process of breaking down text into smaller units called tokens. In NLP, tokens typically represent words, phrases, or sentences. Tokenization is the first step in many NLP tasks, as it converts unstructured text into a structured format that algorithms can process. For example, the sentence “Hello, world!” can be tokenized into [“Hello”, “,”, “world”, “!”]. Tokenization can be done at different levels, such as word-level, sentence-level, or even character-level, depending on the task.\nNormalizing Word Format Word normalization is the process of transforming words into a standard format, which helps reduce the complexity of text data and improve the performance of NLP models. Common normalization techniques include:\nLowercasing: Converting all words to lowercase to ensure that words like “Apple” and “apple” are treated as the same word. Stemming: Reducing words to their base or root form (e.g., “running” becomes “run”). Lemmatization: Converting words to their base form (lemma) based on the word’s meaning and context (e.g., “better” becomes “good”). Normalization helps in reducing variations and inconsistencies in text data, leading to more accurate analysis and modeling.\nSegmenting Sentences in Running Text Sentence segmentation is the process of dividing a running text into individual sentences. This step is crucial in NLP because many tasks, such as sentiment analysis, summarization, and translation, require sentence-level processing. Sentence segmentation typically involves identifying sentence boundaries using punctuation marks like periods, exclamation points, and question marks. However, it can be challenging due to variations in punctuation usage and the presence of abbreviations. Advanced sentence segmentation algorithms use both rule-based and machine learning approaches to accurately detect sentence boundaries.\n10. Text Corpora in NLP What is a Text Corpus? A text corpus (plural: corpora) is a large and structured set of texts used for linguistic research and NLP tasks. Corpora are used to train and evaluate NLP models, providing a rich source of language data. They can contain text from various domains, such as news articles, books, academic papers, or social media posts. Some corpora are annotated with linguistic information, such as part-of-speech tags, syntactic structures, or semantic roles, which makes them valuable for supervised learning tasks in NLP.\nWhat is the Brown Corpus in Python? The Brown Corpus is one of the earliest and most well-known text corpora in the field of computational linguistics. It consists of 1.15 million words from a wide range of genres, including fiction, news, and academic writing, all collected from American English texts published in 1961. The Brown Corpus is fully annotated with part-of-speech tags, making it a valuable resource for training and evaluating NLP models. In Python, the Brown Corpus is available through the Natural Language Toolkit (NLTK) library, where it can be accessed and used for various NLP tasks, such as training taggers or studying language patterns.\nWhat is the Switchboard Corpus? The Switchboard Corpus is a collection of approximately 2,400 telephone conversations between 543 American English speakers, covering a wide range of topics. The conversations were collected in the early 1990s and have been transcribed and annotated with various linguistic features, including part-of-speech tags, syntactic structures, and discourse markers. The Switchboard Corpus is widely used in NLP research, particularly in the fields of speech recognition, dialog systems, and discourse analysis. It provides a rich dataset for studying conversational speech and language use in informal settings.\n11. Text Classification Text Classification Text classification is the process of assigning predefined categories or labels to a given text. It is a common task in Natural Language Processing (NLP) and can be applied to various applications, such as spam detection, sentiment analysis, topic labeling, and document classification. Text classification involves transforming raw text into a format that machine learning models can process, extracting relevant features, and training a model to recognize patterns associated with different classes. Common algorithms used for text classification include Naive Bayes, Support Vector Machines (SVM), and neural networks.\n12. Naive Bayes in Text Classification Naive Bayes in Text Classification Naive Bayes is a simple yet effective probabilistic algorithm used for text classification. It is based on Bayes’ theorem and assumes that the features (e.g., words in a text) are independent given the class label. This “naive” independence assumption makes the model computationally efficient and easy to implement, though it may not always hold true in practice. Despite this, Naive Bayes often performs well in text classification tasks, especially when dealing with large datasets. It is commonly used for tasks like spam detection and sentiment analysis.\n13. Bag of Words in Naive Bayes Bag of Words in Naive Bayes The Bag of Words (BoW) model is a feature extraction technique used in text classification and other NLP tasks. It represents text as a collection (or “bag”) of words, ignoring grammar and word order but keeping track of word frequencies. In the context of Naive Bayes, the BoW model is used to convert text into numerical features that the algorithm can process. Each word in the vocabulary becomes a feature, and its value is typically the word’s frequency or presence in a document. This approach simplifies text into a form that can be fed into a machine learning model.\n14. Naive Bayes Example Naive Bayes Example Consider a simple example of classifying emails as “spam” or “not spam” using Naive Bayes. Let’s say we have a training dataset with labeled emails. First, we build a vocabulary from the training data and use the Bag of Words model to represent each email as a vector of word frequencies. Then, we calculate the probability of each word occurring in spam and not spam emails using the training data. When a new email arrives, Naive Bayes applies Bayes’ theorem to calculate the probability that the email belongs to each class and assigns it to the class with the highest probability.\n15. Evaluation Metrics: Precision, Recall, F-Measure Evaluation: Precision, Recall, F-Measure Evaluation metrics are used to assess the performance of a classification model. The most common metrics include:\nPrecision: The proportion of true positive predictions among all positive predictions. Precision answers the question, “Of all the instances the model predicted as positive, how many were actually positive?” Recall (Sensitivity): The proportion of true positive predictions among all actual positive instances. Recall answers the question, “Of all the actual positive instances, how many did the model correctly identify?” F-Measure (F1-Score): The harmonic mean of precision and recall, providing a single metric that balances both. It is useful when the classes are imbalanced. 16. Confusion Matrix Confusion Matrix A confusion matrix is a table used to describe the performance of a classification model. It provides a breakdown of the model’s predictions into four categories:\nTrue Positives (TP): Correctly predicted positive instances. True Negatives (TN): Correctly predicted negative instances. False Positives (FP): Incorrectly predicted positive instances (Type I error). False Negatives (FN): Incorrectly predicted negative instances (Type II error). The confusion matrix is a powerful tool for visualizing the performance of a model and calculating various metrics like accuracy, precision, recall, and F1-score. 17. Formulas for Precision, Recall, Accuracy, F-Measure Formulas for Precision, Recall, Accuracy, F-Measure The formulas for the key evaluation metrics are as follows:\nPrecision: ( \\text{Precision} = \\frac{TP}{TP + FP} ) Recall (Sensitivity): ( \\text{Recall} = \\frac{TP}{TP + FN} ) Accuracy: ( \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} ) F-Measure (F1-Score): ( \\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} ) These metrics provide different perspectives on the model’s performance, helping to identify strengths and weaknesses in different aspects of prediction.\n18. Logistic Regression for Text Sentiment Analysis How to Use Logistic Regression for Text Sentiment Analysis Logistic Regression is a linear model used for binary classification, making it suitable for sentiment analysis tasks where the goal is to classify text as having positive or negative sentiment. In this context, the steps include:\nFeature Extraction: Convert text into numerical features using techniques like Bag of Words or TF-IDF. Model Training: Train the Logistic Regression model on labeled sentiment data, where each text is labeled as positive or negative. Prediction: Apply the trained model to new, unseen text to predict the sentiment. Logistic Regression outputs a probability that the text belongs to a certain class, with a threshold used to make the final classification decision. 19. Language Models Language Models A language model is a probabilistic model that assigns a probability to a sequence of words in a language. It predicts the likelihood of a word given the previous words in the sequence. Language models are essential in NLP tasks such as speech recognition, machine translation, and text generation. There are different types of language models, including n-gram models, neural language models, and transformer-based models like GPT.\n20. Unigram Model Unigram Model A unigram model is the simplest type of language model that assumes each word in a sequence is independent of the others. It calculates the probability of a word based on its frequency in the training corpus. For example, the probability of a sentence is the product of the probabilities of each word occurring independently. While simplistic, unigram models are useful for tasks like document classification and word prediction when combined with other models.\n21. Bigram Model Bigram Model A bigram model is a type of language model that considers the probability of a word based on the preceding word in the sequence. It captures word dependencies and calculates the probability of a word given the previous word. For example, the probability of the word “morning” following the word “good” would be calculated based on their co-occurrence in the training corpus. Bigram models are more accurate than unigram models but still have limitations in capturing longer dependencies.\n22. N-Gram Model N-Gram Model An n-gram model is a generalization of unigram and bigram models that predicts the probability of a word based on the previous (n-1) words. For example, a trigram model (n=3) predicts a word based on the two preceding words. N-gram models capture more context than unigram and bigram models but require larger datasets to estimate probabilities accurately. As n increases, the model becomes more context-aware but also more computationally expensive.\n23. Zero Probability in Bigrams Zero Probability Bigrams Zero probability in bigrams occurs when a bigram (a pair of words) is not observed in the training corpus, leading the model to assign it a probability of zero. This is problematic because it means the model will reject any sequence containing that bigram, even if it is a plausible or correct sequence. This issue arises because the bigram model relies on observed frequencies to estimate probabilities.\n24. Solution to Zero Probability Bigrams (Smoothing) Solution to Zero Probability Bigrams (Smoothing) Smoothing techniques are used to address the problem of zero probabilities in language models. Smoothing adjusts the estimated probabilities to account for unseen events (e.g., bigrams that did not appear in the training data). Common smoothing methods include:\nLaplace Smoothing: Adds a small constant (usually 1) to all counts to ensure that no probability is zero. Good-Turing Smoothing: Adjusts the probabilities of unseen events based on the frequency of events that were seen once. These techniques help the model generalize better to unseen data and avoid assigning zero probabilities to valid word sequences. 25. Laplace Smoothing Laplace Smoothing Laplace Smoothing, also known as add-one smoothing, is a simple technique used to handle zero probabilities in language models. It works by adding a count of 1 to every possible n-gram, ensuring that even n-grams not seen in the training data have a small non-zero probability. The formula for Laplace smoothing in a bigram model is: [ P(w_n | w_{n-1}) = \\frac{\\text{count}(w_{n-1}, w_n) + 1}{\\text{count}(w_{n-1}) + V} ] where ( V ) is the size of the vocabulary. This technique helps prevent the model from assigning zero probability to unseen word pairs.\n26. Backoff and Interpolation Backoff and Interpolation Backoff and interpolation are techniques used in n-gram models to handle cases where higher-order n-grams (e.g., trigrams) have zero counts or low confidence.\nBackoff: The model “backs off” to a lower-order n-gram (e.g., bigram) when the higher-order n-gram has a zero count. This ensures that the model can still make a prediction even when some word combinations are unseen. Interpolation: The model combines the probabilities from higher-order and lower-order n-grams by weighting them, rather than backing off completely. The weights are usually determined based on the confidence in the higher-order n-gram counts. These techniques improve the robustness of language models by ensuring that probabilities are more reliable even with sparse data. 27. Linear Interpolation Linear Interpolation Linear interpolation is a method used in n-gram language models to combine the probabilities of different n-gram levels (e.g., unigram, bigram, trigram) into a single probability estimate. The formula for linear interpolation in a trigram model might look like: [ P(w_n | w_{n-1}, w_{n-2}) = \\lambda_3 P(w_n | w_{n-1}, w_{n-2}) + \\lambda_2 P(w_n | w_{n-1}) + \\lambda_1 P(w_n) ] where ( \\lambda_1 + \\lambda_2 + \\lambda_3 = 1 ) are the interpolation weights. This method helps the model balance the use of context from different levels of n-grams, making it more flexible and accurate.\n28. Embedding Methods (TF-IDF and Word2Vec) Embedding Methods (TF-IDF and Word2Vec) Embedding methods are techniques used to represent words or documents as vectors in a continuous space. These methods capture the semantic meaning of words and their relationships.\nTF-IDF (Term Frequency-Inverse Document Frequency): A statistical measure used to evaluate the importance of a word in a document relative to a corpus. It combines the frequency of a word in a document (TF) with the inverse frequency of the word across all documents (IDF). TF-IDF is useful for tasks like information retrieval and text classification. Word2Vec: A neural network-based model that learns dense vector representations (embeddings) of words based on their context in a large corpus. Word2Vec captures semantic relationships between words, allowing similar words to have similar vectors. It is widely used in NLP tasks like word similarity, analogy reasoning, and text classification. 29. Term-to-Document Matrix Term-to-Document Matrix A term-to-document matrix is a mathematical representation of a corpus where rows represent terms (words) and columns represent documents. Each cell in the matrix contains a value representing the frequency or weight of the term in the document. This matrix is used in text mining and information retrieval tasks, where it forms the basis for techniques like TF-IDF and Latent Semantic Analysis (LSA). The matrix can be sparse, especially for large corpora, and is often processed using dimensionality reduction techniques to make it more manageable.\n30. Word-to-Word Matrix Word-to-Word Matrix A word-to-word matrix is a matrix where both rows and columns represent words, and each cell indicates the relationship or co-occurrence between the two words. This matrix is used in various NLP tasks to capture word associations and similarities. For example, in a co-occurrence matrix, the value in a cell might represent how often two words appear together in a corpus. Word-to-word matrices are used in models like Word2Vec and GloVe to learn word embeddings that capture semantic relationships between words.\n31. TF-IDF Formulas TF-IDF Formulas The TF-IDF value for a term ( t ) in a document ( d ) is calculated using the following formulas:\nTerm Frequency (TF): Measures how frequently a term appears in a document. [ \\text{TF}(t, d) = \\frac{\\text{count}(t, d)}{\\text{total terms in } d} ] Inverse Document Frequency (IDF): Measures how important a term is across all documents in the corpus. [ \\text{IDF}(t) = \\log\\left(\\frac{\\text{total documents}}{\\text{documents containing } t}\\right) ] TF-IDF: The product of TF and IDF, giving a weight that reflects the term’s importance in the document relative to the corpus. [ \\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t) ] TF-IDF is commonly used to convert text into numerical features for machine learning models. 32. Pointwise Mutual Information Pointwise Mutual Information Pointwise Mutual Information (PMI) is a measure of association between two words or events. It quantifies how much more likely two words are to appear together than if they were independent. The formula for PMI between two words ( w_1 ) and ( w_2 ) is: [ \\text{PMI}(w_1, w_2) = \\log\\left(\\frac{P(w_1, w_2)}{P(w_1) \\times P(w_2)}\\right) ] where ( P(w_1, w_2) ) is the joint probability of the two words, and ( P(w_1) ) and ( P(w_2) ) are their individual probabilities. PMI is often used in NLP to identify word associations and semantic similarity.\n33. TF-IDF Cons (Too Many Dimensions for Each Vector and More) TF-IDF Cons (Too Many Dimensions for Each Vector and More) While TF-IDF is a powerful tool for text representation, it has several limitations:\nHigh Dimensionality: The number of features (terms) in the TF-IDF matrix can be very large, leading to a high-dimensional space that is computationally expensive to process. Sparsity: The TF-IDF matrix is often sparse, meaning many cells contain zeros. This sparsity can make it challenging to perform efficient computations and may require dimensionality reduction techniques. Lack of Semantic Understanding: TF-IDF does not capture the semantic meaning of words or their relationships, treating each term as independent of others. This limitation can lead to suboptimal performance in tasks that require understanding of word meaning. 34. Word2Vec Word2Vec Word2Vec is a neural network-based model that learns continuous vector representations of words by analyzing large text corpora. The vectors, also known as word embeddings, capture semantic relationships between words, allowing similar words to have similar vector representations. Word2Vec has two main architectures:\nSkip-Gram: Predicts the context words given a target word. Continuous Bag of Words (CBOW): Predicts the target word given its context words. Word2Vec has been widely adopted in NLP tasks such as word similarity, analogy reasoning, and as input to more complex models like neural networks. 35. Hidden Markov Model (HMM) Hidden Markov Model (HMM) A Hidden Markov Model (HMM) is a statistical model that represents systems with hidden (unobservable) states. It is used in various sequential data tasks, including speech recognition, part-of-speech tagging, and bioinformatics. An HMM consists of:\nStates: The hidden states that the system can be in. Observations: The visible outputs generated by the system, which depend on the hidden states. Transition Probabilities: The probabilities of transitioning from one state to another. Emission Probabilities: The probabilities of observing a particular output given a state. HMMs are used to model processes where the system’s true state is not directly observable, but inferences can be made based on the observed data. 36. Transition Probability Matrix in HMM Transition Probability Matrix The transition probability matrix in an HMM defines the probabilities of transitioning from one hidden state to another. Each entry ( a_{ij} ) in the matrix represents the probability of transitioning from state ( i ) to state ( j ): [ A = [a_{ij}] \\text{ where } a_{ij} = P(S_{t+1} = j | S_t = i) ] This matrix is crucial for determining the likelihood of sequences of states and is used in algorithms like the Viterbi algorithm to decode the most likely sequence of hidden states given a sequence of observations.\n37. States in HMM States in HMM In a Hidden Markov Model, the states represent the hidden (unobservable) conditions or categories that the system can be in at any given time. These states are not directly observable, but they influence the observable outputs or emissions. The sequence of states over time is modeled as a Markov process, where the probability of transitioning to a new state depends only on the current state, not on previous states. The states in an HMM are fundamental to understanding the system’s behavior and making predictions based on observed data.\n38. Decoding in HMM Decoding in HMM Decoding in HMM refers to the process of determining the most likely sequence of hidden states given a sequence of observations. The most common algorithm used for decoding is the Viterbi algorithm, which finds the single most likely sequence of states by dynamically computing the probability of the most likely path to each state at each time step. Decoding is essential for tasks like speech recognition and part-of-speech tagging, where the goal is to infer the underlying state sequence\n","wordCount":"6354","inLanguage":"en","datePublished":"2017-03-02T12:00:00-05:00","dateModified":"2017-03-02T12:00:00-05:00","author":{"@type":"Person","name":"Sajad"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://sajadabdollahi.ir/posts/nlp-brief/"},"publisher":{"@type":"Organization","name":"Sajad Abdollahi","logo":{"@type":"ImageObject","url":"https://sajadabdollahi.ir/icons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://sajadabdollahi.ir/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://sajadabdollahi.ir/fa/ title=Farsi aria-label=fa>Fa</a></li></ul></div></div><ul id=menu><li><a href=https://sajadabdollahi.ir/about/ title=About><span>About</span></a></li><li><a href=https://sajadabdollahi.ir/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://sajadabdollahi.ir/>Home</a>&nbsp;»&nbsp;<a href=https://sajadabdollahi.ir/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">NLP basics</h1><div class=post-meta><span title='2017-03-02 12:00:00 -0500 -0500'>March 2, 2017</span>&nbsp;·&nbsp;30 min&nbsp;·&nbsp;Sajad</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-machine-learning-basics aria-label="1. Machine Learning Basics">1. Machine Learning Basics</a><ul><li><a href=#supervised-learning aria-label="Supervised Learning">Supervised Learning</a></li><li><a href=#unsupervised-learning aria-label="Unsupervised Learning">Unsupervised Learning</a></li><li><a href=#regression aria-label=Regression>Regression</a></li><li><a href=#classification aria-label=Classification>Classification</a></li></ul></li><li><a href=#2-optimization-in-machine-learning aria-label="2. Optimization in Machine Learning">2. Optimization in Machine Learning</a><ul><li><a href=#learning-rate-alpha aria-label="Learning Rate (Alpha)">Learning Rate (Alpha)</a></li><li><a href=#gradient-descent aria-label="Gradient Descent">Gradient Descent</a></li><li><a href=#stochastic-gradient-descent-sgd aria-label="Stochastic Gradient Descent (SGD)">Stochastic Gradient Descent (SGD)</a></li><li><a href=#batch-gradient-descent aria-label="Batch Gradient Descent">Batch Gradient Descent</a></li><li><a href=#mini-batch-gradient-descent aria-label="Mini-Batch Gradient Descent">Mini-Batch Gradient Descent</a></li></ul></li><li><a href=#3-regression-techniques aria-label="3. Regression Techniques">3. Regression Techniques</a><ul><li><a href=#linear-regression aria-label="Linear Regression">Linear Regression</a></li><li><a href=#logistic-regression aria-label="Logistic Regression">Logistic Regression</a></li><li><a href=#logistic-regression-for-multi-class-problems aria-label="Logistic Regression for Multi-Class Problems">Logistic Regression for Multi-Class Problems</a></li></ul></li><li><a href=#4-regularization aria-label="4. Regularization">4. Regularization</a><ul><li><a href=#regularization-a-solution-to-overfitting aria-label="Regularization (A Solution to Overfitting)">Regularization (A Solution to Overfitting)</a></li><li><a href=#regularization-rate-lambda aria-label="Regularization Rate (Lambda)">Regularization Rate (Lambda)</a></li></ul></li><li><a href=#5-neural-networks-nn aria-label="5. Neural Networks (NN)">5. Neural Networks (NN)</a><ul><li><a href=#neural-network aria-label="Neural Network">Neural Network</a></li><li><a href=#non-linear-classification aria-label="Non-Linear Classification">Non-Linear Classification</a></li><li><a href=#xnor-neural-network aria-label="XNOR Neural Network">XNOR Neural Network</a></li><li><a href=#nn-for-classification aria-label="NN for Classification">NN for Classification</a></li><li><a href=#nn-for-regression aria-label="NN for Regression">NN for Regression</a></li><li><a href=#cost-function aria-label="Cost Function">Cost Function</a></li><li><a href=#backpropagation aria-label=Backpropagation>Backpropagation</a></li><li><a href=#neurons-count-in-neural-network aria-label="Neurons Count in Neural Network">Neurons Count in Neural Network</a></li><li><a href=#layers-count-in-neural-network aria-label="Layers Count in Neural Network">Layers Count in Neural Network</a></li><li><a href=#steps-to-create-a-neural-network aria-label="Steps to Create a Neural Network">Steps to Create a Neural Network</a></li></ul></li><li><a href=#6-evaluating-model-performance aria-label="6. Evaluating Model Performance">6. Evaluating Model Performance</a><ul><li><a href=#evaluate-hypothesis aria-label="Evaluate Hypothesis">Evaluate Hypothesis</a></li><li><a href=#overfitting aria-label=Overfitting>Overfitting</a></li><li><a href=#training-error aria-label="Training Error">Training Error</a></li><li><a href=#cross-validation-error aria-label="Cross-Validation Error">Cross-Validation Error</a></li><li><a href=#test-error aria-label="Test Error">Test Error</a></li></ul></li><li><a href=#7-bias-and-variance-tradeoff aria-label="7. Bias and Variance Tradeoff">7. Bias and Variance Tradeoff</a><ul><li><a href=#underfitting-bias aria-label="Underfitting: Bias">Underfitting: Bias</a></li><li><a href=#overfitting-variance aria-label="Overfitting: Variance">Overfitting: Variance</a></li></ul></li><li><a href=#8-regular-expressions-regex-in-nlp aria-label="8. Regular Expressions (Regex) in NLP">8. Regular Expressions (Regex) in NLP</a><ul><li><a href=#regex aria-label=Regex>Regex</a></li><li><a href=#regex-usage-in-nlp aria-label="Regex Usage in NLP">Regex Usage in NLP</a></li><li><a href=#common-regexes aria-label="Common Regexes">Common Regexes</a></li></ul></li><li><a href=#9-nlp-steps aria-label="9. NLP Steps">9. NLP Steps</a><ul><li><a href=#tokenizing aria-label=Tokenizing>Tokenizing</a></li><li><a href=#normalizing-word-format aria-label="Normalizing Word Format">Normalizing Word Format</a></li><li><a href=#segmenting-sentences-in-running-text aria-label="Segmenting Sentences in Running Text">Segmenting Sentences in Running Text</a></li></ul></li><li><a href=#10-text-corpora-in-nlp aria-label="10. Text Corpora in NLP">10. Text Corpora in NLP</a><ul><li><a href=#what-is-a-text-corpus aria-label="What is a Text Corpus?">What is a Text Corpus?</a></li><li><a href=#what-is-the-brown-corpus-in-python aria-label="What is the Brown Corpus in Python?">What is the Brown Corpus in Python?</a></li><li><a href=#what-is-the-switchboard-corpus aria-label="What is the Switchboard Corpus?">What is the Switchboard Corpus?</a></li></ul></li><li><a href=#11-text-classification aria-label="11. Text Classification">11. Text Classification</a><ul><li><a href=#text-classification aria-label="Text Classification">Text Classification</a></li></ul></li><li><a href=#12-naive-bayes-in-text-classification aria-label="12. Naive Bayes in Text Classification">12. Naive Bayes in Text Classification</a><ul><li><a href=#naive-bayes-in-text-classification aria-label="Naive Bayes in Text Classification">Naive Bayes in Text Classification</a></li></ul></li><li><a href=#13-bag-of-words-in-naive-bayes aria-label="13. Bag of Words in Naive Bayes">13. Bag of Words in Naive Bayes</a><ul><li><a href=#bag-of-words-in-naive-bayes aria-label="Bag of Words in Naive Bayes">Bag of Words in Naive Bayes</a></li></ul></li><li><a href=#14-naive-bayes-example aria-label="14. Naive Bayes Example">14. Naive Bayes Example</a><ul><li><a href=#naive-bayes-example aria-label="Naive Bayes Example">Naive Bayes Example</a></li></ul></li><li><a href=#15-evaluation-metrics-precision-recall-f-measure aria-label="15. Evaluation Metrics: Precision, Recall, F-Measure">15. Evaluation Metrics: Precision, Recall, F-Measure</a><ul><li><a href=#evaluation-precision-recall-f-measure aria-label="Evaluation: Precision, Recall, F-Measure">Evaluation: Precision, Recall, F-Measure</a></li></ul></li><li><a href=#16-confusion-matrix aria-label="16. Confusion Matrix">16. Confusion Matrix</a><ul><li><a href=#confusion-matrix aria-label="Confusion Matrix">Confusion Matrix</a></li></ul></li><li><a href=#17-formulas-for-precision-recall-accuracy-f-measure aria-label="17. Formulas for Precision, Recall, Accuracy, F-Measure">17. Formulas for Precision, Recall, Accuracy, F-Measure</a><ul><li><a href=#formulas-for-precision-recall-accuracy-f-measure aria-label="Formulas for Precision, Recall, Accuracy, F-Measure">Formulas for Precision, Recall, Accuracy, F-Measure</a></li></ul></li><li><a href=#18-logistic-regression-for-text-sentiment-analysis aria-label="18. Logistic Regression for Text Sentiment Analysis">18. Logistic Regression for Text Sentiment Analysis</a><ul><li><a href=#how-to-use-logistic-regression-for-text-sentiment-analysis aria-label="How to Use Logistic Regression for Text Sentiment Analysis">How to Use Logistic Regression for Text Sentiment Analysis</a></li></ul></li><li><a href=#19-language-models aria-label="19. Language Models">19. Language Models</a><ul><li><a href=#language-models aria-label="Language Models">Language Models</a></li></ul></li><li><a href=#20-unigram-model aria-label="20. Unigram Model">20. Unigram Model</a><ul><li><a href=#unigram-model aria-label="Unigram Model">Unigram Model</a></li></ul></li><li><a href=#21-bigram-model aria-label="21. Bigram Model">21. Bigram Model</a><ul><li><a href=#bigram-model aria-label="Bigram Model">Bigram Model</a></li></ul></li><li><a href=#22-n-gram-model aria-label="22. N-Gram Model">22. N-Gram Model</a><ul><li><a href=#n-gram-model aria-label="N-Gram Model">N-Gram Model</a></li></ul></li><li><a href=#23-zero-probability-in-bigrams aria-label="23. Zero Probability in Bigrams">23. Zero Probability in Bigrams</a><ul><li><a href=#zero-probability-bigrams aria-label="Zero Probability Bigrams">Zero Probability Bigrams</a></li></ul></li><li><a href=#24-solution-to-zero-probability-bigrams-smoothing aria-label="24. Solution to Zero Probability Bigrams (Smoothing)">24. Solution to Zero Probability Bigrams (Smoothing)</a><ul><li><a href=#solution-to-zero-probability-bigrams-smoothing aria-label="Solution to Zero Probability Bigrams (Smoothing)">Solution to Zero Probability Bigrams (Smoothing)</a></li></ul></li><li><a href=#25-laplace-smoothing aria-label="25. Laplace Smoothing">25. Laplace Smoothing</a><ul><li><a href=#laplace-smoothing aria-label="Laplace Smoothing">Laplace Smoothing</a></li></ul></li><li><a href=#26-backoff-and-interpolation aria-label="26. Backoff and Interpolation">26. Backoff and Interpolation</a><ul><li><a href=#backoff-and-interpolation aria-label="Backoff and Interpolation">Backoff and Interpolation</a></li></ul></li><li><a href=#27-linear-interpolation aria-label="27. Linear Interpolation">27. Linear Interpolation</a><ul><li><a href=#linear-interpolation aria-label="Linear Interpolation">Linear Interpolation</a></li></ul></li><li><a href=#28-embedding-methods-tf-idf-and-word2vec aria-label="28. Embedding Methods (TF-IDF and Word2Vec)">28. Embedding Methods (TF-IDF and Word2Vec)</a><ul><li><a href=#embedding-methods-tf-idf-and-word2vec aria-label="Embedding Methods (TF-IDF and Word2Vec)">Embedding Methods (TF-IDF and Word2Vec)</a></li></ul></li><li><a href=#29-term-to-document-matrix aria-label="29. Term-to-Document Matrix">29. Term-to-Document Matrix</a><ul><li><a href=#term-to-document-matrix aria-label="Term-to-Document Matrix">Term-to-Document Matrix</a></li></ul></li><li><a href=#30-word-to-word-matrix aria-label="30. Word-to-Word Matrix">30. Word-to-Word Matrix</a><ul><li><a href=#word-to-word-matrix aria-label="Word-to-Word Matrix">Word-to-Word Matrix</a></li></ul></li><li><a href=#31-tf-idf-formulas aria-label="31. TF-IDF Formulas">31. TF-IDF Formulas</a><ul><li><a href=#tf-idf-formulas aria-label="TF-IDF Formulas">TF-IDF Formulas</a></li></ul></li><li><a href=#32-pointwise-mutual-information aria-label="32. Pointwise Mutual Information">32. Pointwise Mutual Information</a><ul><li><a href=#pointwise-mutual-information aria-label="Pointwise Mutual Information">Pointwise Mutual Information</a></li></ul></li><li><a href=#33-tf-idf-cons-too-many-dimensions-for-each-vector-and-more aria-label="33. TF-IDF Cons (Too Many Dimensions for Each Vector and More)">33. TF-IDF Cons (Too Many Dimensions for Each Vector and More)</a><ul><li><a href=#tf-idf-cons-too-many-dimensions-for-each-vector-and-more aria-label="TF-IDF Cons (Too Many Dimensions for Each Vector and More)">TF-IDF Cons (Too Many Dimensions for Each Vector and More)</a></li></ul></li><li><a href=#34-word2vec aria-label="34. Word2Vec">34. Word2Vec</a><ul><li><a href=#word2vec aria-label=Word2Vec>Word2Vec</a></li></ul></li><li><a href=#35-hidden-markov-model-hmm aria-label="35. Hidden Markov Model (HMM)">35. Hidden Markov Model (HMM)</a><ul><li><a href=#hidden-markov-model-hmm aria-label="Hidden Markov Model (HMM)">Hidden Markov Model (HMM)</a></li></ul></li><li><a href=#36-transition-probability-matrix-in-hmm aria-label="36. Transition Probability Matrix in HMM">36. Transition Probability Matrix in HMM</a><ul><li><a href=#transition-probability-matrix aria-label="Transition Probability Matrix">Transition Probability Matrix</a></li></ul></li><li><a href=#37-states-in-hmm aria-label="37. States in HMM">37. States in HMM</a><ul><li><a href=#states-in-hmm aria-label="States in HMM">States in HMM</a></li></ul></li><li><a href=#38-decoding-in-hmm aria-label="38. Decoding in HMM">38. Decoding in HMM</a><ul><li><a href=#decoding-in-hmm aria-label="Decoding in HMM">Decoding in HMM</a></li></ul></li></ul></div></details></div><div class=post-content><p>This guide provides an overview of text classification, exploring essential methods such as Naive Bayes, Logistic Regression, and Hidden Markov Models (HMMs). We’ll also cover critical concepts like n-gram models, embedding methods like TF-IDF and Word2Vec, and evaluation metrics including precision, recall, and F-measure. Whether you’re a beginner seeking to grasp the basics or an experienced practitioner looking to refine your skills, this guide will equip you with the knowledge to effectively tackle text classification tasks.</p><h2 id=1-machine-learning-basics><strong>1. Machine Learning Basics</strong><a hidden class=anchor aria-hidden=true href=#1-machine-learning-basics>#</a></h2><h3 id=supervised-learning><strong>Supervised Learning</strong><a hidden class=anchor aria-hidden=true href=#supervised-learning>#</a></h3><p>Supervised learning is a type of machine learning where the model is trained on a labeled dataset. This means that for each example in the training set, the input data comes with a corresponding correct output (label). The model&rsquo;s goal is to learn a mapping from inputs to outputs so that it can accurately predict the output for new, unseen data. Supervised learning is used in a wide range of applications, including image recognition, spam detection, and medical diagnosis. Common algorithms include linear regression, decision trees, and support vector machines.</p><h3 id=unsupervised-learning><strong>Unsupervised Learning</strong><a hidden class=anchor aria-hidden=true href=#unsupervised-learning>#</a></h3><p>Unsupervised learning involves training a model on data without labeled outputs. The model tries to learn the underlying structure of the data by identifying patterns, relationships, or groupings. Clustering and dimensionality reduction are two common tasks in unsupervised learning. For example, clustering algorithms like K-means can be used to group customers into segments based on purchasing behavior, while dimensionality reduction techniques like PCA (Principal Component Analysis) are used to simplify datasets by reducing the number of features.</p><h3 id=regression><strong>Regression</strong><a hidden class=anchor aria-hidden=true href=#regression>#</a></h3><p>Regression is a type of supervised learning where the goal is to predict a continuous output variable (also known as the dependent variable) based on one or more input variables (independent variables). The simplest form of regression is linear regression, where the relationship between the input and output is modeled as a straight line. Regression models are widely used in forecasting (e.g., predicting sales, stock prices) and determining the strength of relationships between variables.</p><h3 id=classification><strong>Classification</strong><a hidden class=anchor aria-hidden=true href=#classification>#</a></h3><p>Classification is another type of supervised learning where the goal is to assign input data to one of several predefined categories or classes. Unlike regression, where the output is continuous, the output in classification is discrete. For example, a spam filter classifies emails as either &ldquo;spam&rdquo; or &ldquo;not spam.&rdquo; Common algorithms for classification include logistic regression, decision trees, and support vector machines. In multi-class classification, the model predicts which one of several possible classes an instance belongs to.</p><h2 id=2-optimization-in-machine-learning><strong>2. Optimization in Machine Learning</strong><a hidden class=anchor aria-hidden=true href=#2-optimization-in-machine-learning>#</a></h2><h3 id=learning-rate-alpha><strong>Learning Rate (Alpha)</strong><a hidden class=anchor aria-hidden=true href=#learning-rate-alpha>#</a></h3><p>The learning rate, often denoted by the Greek letter alpha (α), is a crucial hyperparameter in the training of machine learning models. It controls the size of the steps taken by the optimization algorithm (such as gradient descent) when adjusting the model&rsquo;s weights. If the learning rate is too high, the model might overshoot the optimal solution, leading to divergence or instability. If the learning rate is too low, the training process can become very slow, and the model may get stuck in local minima. Finding the right learning rate is essential for efficient and effective training.</p><h3 id=gradient-descent><strong>Gradient Descent</strong><a hidden class=anchor aria-hidden=true href=#gradient-descent>#</a></h3><p>Gradient Descent is an iterative optimization algorithm used to minimize the cost function in machine learning models. The cost function measures how well the model&rsquo;s predictions match the actual data. Gradient Descent works by calculating the gradient (partial derivative) of the cost function concerning the model&rsquo;s parameters (weights). The model parameters are then updated in the opposite direction of the gradient, hence the term &ldquo;descent.&rdquo; This process is repeated until the model converges to a minimum point in the cost function, ideally the global minimum.</p><h3 id=stochastic-gradient-descent-sgd><strong>Stochastic Gradient Descent (SGD)</strong><a hidden class=anchor aria-hidden=true href=#stochastic-gradient-descent-sgd>#</a></h3><p>Stochastic Gradient Descent (SGD) is a variation of the gradient descent algorithm. Unlike traditional gradient descent, which computes the gradient using the entire dataset, SGD updates the model parameters after each training example. This makes SGD faster and more suitable for large datasets, but it introduces more noise into the training process, leading to more fluctuations in the cost function. However, this noise can help the model escape local minima and potentially find a better overall solution.</p><h3 id=batch-gradient-descent><strong>Batch Gradient Descent</strong><a hidden class=anchor aria-hidden=true href=#batch-gradient-descent>#</a></h3><p>Batch Gradient Descent is the traditional form of gradient descent, where the gradient of the cost function is computed using the entire dataset before updating the model&rsquo;s parameters. This method provides a smooth convergence path, as the gradient calculation is based on the overall direction of the dataset. However, it can be computationally expensive and slow, especially for large datasets, as it requires loading the entire dataset into memory and performing the gradient calculation for each parameter update.</p><h3 id=mini-batch-gradient-descent><strong>Mini-Batch Gradient Descent</strong><a hidden class=anchor aria-hidden=true href=#mini-batch-gradient-descent>#</a></h3><p>Mini-Batch Gradient Descent is a compromise between Stochastic Gradient Descent and Batch Gradient Descent. It splits the dataset into small, manageable batches and updates the model&rsquo;s parameters for each batch. This method balances the benefits of both SGD and Batch Gradient Descent. It offers the efficiency and speed of SGD while reducing the noise in the updates, leading to a smoother convergence path. Mini-Batch Gradient Descent is widely used in practice, particularly in training neural networks.</p><h2 id=3-regression-techniques><strong>3. Regression Techniques</strong><a hidden class=anchor aria-hidden=true href=#3-regression-techniques>#</a></h2><h3 id=linear-regression><strong>Linear Regression</strong><a hidden class=anchor aria-hidden=true href=#linear-regression>#</a></h3><p>Linear Regression is one of the simplest and most commonly used regression algorithms. It models the relationship between a dependent variable (output) and one or more independent variables (inputs) as a linear equation. The equation takes the form:</p><p>[ y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_nx_n ]</p><p>where ( y ) is the predicted output, ( x_1, x_2, \dots, x_n ) are the input variables, and ( \beta_0, \beta_1, \dots, \beta_n ) are the coefficients (weights) that the model learns during training. The goal of linear regression is to find the line (or hyperplane in higher dimensions) that best fits the data by minimizing the difference between the predicted values and the actual values. Linear regression is often used in forecasting and understanding the relationship between variables.</p><h3 id=logistic-regression><strong>Logistic Regression</strong><a hidden class=anchor aria-hidden=true href=#logistic-regression>#</a></h3><p>Logistic Regression is a classification algorithm used for binary classification tasks, where the output variable can take on two possible values (e.g., yes/no, 0/1, true/false). Instead of predicting a continuous output, logistic regression predicts the probability that an instance belongs to a particular class. It uses the logistic function (also known as the sigmoid function) to map the predicted values to probabilities between 0 and 1. The output is then classified based on a threshold, usually 0.5. Despite its name, logistic regression is primarily used for classification rather than regression.</p><h3 id=logistic-regression-for-multi-class-problems><strong>Logistic Regression for Multi-Class Problems</strong><a hidden class=anchor aria-hidden=true href=#logistic-regression-for-multi-class-problems>#</a></h3><p>While logistic regression is inherently a binary classifier, it can be extended to handle multi-class classification problems through techniques such as One-vs-Rest (OvR) or Softmax Regression.</p><ul><li><strong>One-vs-Rest (OvR):</strong> In this approach, multiple binary classifiers are trained, one for each class. Each classifier distinguishes one class from all the others, and the final prediction is based on the classifier with the highest confidence.</li><li><strong>Softmax Regression:</strong> This is an extension of logistic regression that directly handles multi-class classification. It uses the softmax function to predict the probabilities of each class, and the class with the highest probability is chosen as the prediction.</li></ul><h2 id=4-regularization><strong>4. Regularization</strong><a hidden class=anchor aria-hidden=true href=#4-regularization>#</a></h2><h3 id=regularization-a-solution-to-overfitting><strong>Regularization (A Solution to Overfitting)</strong><a hidden class=anchor aria-hidden=true href=#regularization-a-solution-to-overfitting>#</a></h3><p>Regularization is a technique used to prevent overfitting, which occurs when a model learns the noise and details in the training data to the detriment of its performance on new data. Regularization works by adding a penalty term to the cost function, which discourages the model from becoming too complex. The most common types of regularization are:</p><ul><li><strong>L1 Regularization (Lasso):</strong> Adds the absolute value of the coefficients as a penalty to the cost function. This can lead to sparse models where some feature weights are reduced to zero, effectively performing feature selection.</li><li><strong>L2 Regularization (Ridge):</strong> Adds the square of the coefficients as a penalty to the cost function. This tends to distribute the error among all features, reducing the impact of any one feature on the model&rsquo;s predictions.</li></ul><h3 id=regularization-rate-lambda><strong>Regularization Rate (Lambda)</strong><a hidden class=anchor aria-hidden=true href=#regularization-rate-lambda>#</a></h3><p>The regularization rate, denoted by lambda (λ), controls the strength of the penalty applied during regularization. A higher lambda increases the penalty, leading to a simpler model that may generalize better to new data but may underfit the training data. Conversely, a lower lambda reduces the penalty, allowing the model to capture more complex patterns in the training data, but increasing the risk of overfitting. Tuning the regularization rate is crucial for finding the right balance between bias and variance in the model.</p><h2 id=5-neural-networks-nn><strong>5. Neural Networks (NN)</strong><a hidden class=anchor aria-hidden=true href=#5-neural-networks-nn>#</a></h2><h3 id=neural-network><strong>Neural Network</strong><a hidden class=anchor aria-hidden=true href=#neural-network>#</a></h3><p>A Neural Network is a computational model inspired by the structure and function of the human brain. It consists of layers of interconnected nodes (neurons), where each connection has an associated weight. Neural networks are capable of learning complex patterns and relationships in data through a process called training. They are particularly powerful in tasks like image recognition, natural language processing, and game playing.</p><p>A basic neural network consists of three types of layers:</p><ul><li><strong>Input Layer:</strong> The layer that receives the input data.</li><li><strong>Hidden Layers:</strong> Layers between the input and output layers where the network learns to represent the data. The more hidden layers, the deeper the network, allowing it to learn more complex features.</li><li><strong>Output Layer:</strong> The layer that produces the final output, such as class labels or predicted values.</li></ul><h3 id=non-linear-classification><strong>Non-Linear Classification</strong><a hidden class=anchor aria-hidden=true href=#non-linear-classification>#</a></h3><p>Non-Linear Classification refers to the classification of data that cannot be separated by a straight line or linear boundary. In many real-world scenarios, the relationship between the features and the target variable is non-linear. Neural networks are well-suited for non-linear classification because they can learn complex, non-linear decision boundaries through multiple layers of neurons and non-linear activation functions.</p><h3 id=xnor-neural-network><strong>XNOR Neural Network</strong><a hidden class=anchor aria-hidden=true href=#xnor-neural-network>#</a></h3><p>An XNOR Neural Network is a simple example of a neural network that can solve the XNOR logic problem, which is a non-linear classification problem. The XNOR gate outputs true only when both inputs are the same (either both true or both false). A single-layer neural network cannot solve this problem, but a neural network with at least one hidden layer can, demonstrating the power of non-linear decision boundaries.</p><h3 id=nn-for-classification><strong>NN for Classification</strong><a hidden class=anchor aria-hidden=true href=#nn-for-classification>#</a></h3><p>Neural networks are commonly used for classification tasks, where the goal is to assign input data to one of several categories. In a classification neural network, the output layer typically uses a softmax activation function for multi-class classification, which converts the network&rsquo;s outputs into probabilities. The class with the highest probability is chosen as the predicted category.</p><h3 id=nn-for-regression><strong>NN for Regression</strong><a hidden class=anchor aria-hidden=true href=#nn-for-regression>#</a></h3><p>Neural networks can also be used for regression tasks, where the goal is to predict a continuous output value. In a regression neural network, the output layer typically uses a linear activation function to produce continuous values. The network learns to map input features to a continuous output through training on labeled data.</p><h3 id=cost-function><strong>Cost Function</strong><a hidden class=anchor aria-hidden=true href=#cost-function>#</a></h3><p>The cost function, also known as the loss function, measures the difference between the predicted output of the neural network and the actual output. The goal of training is to minimize this cost function, making the predictions as accurate as possible. Common cost functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.</p><h3 id=backpropagation><strong>Backpropagation</strong><a hidden class=anchor aria-hidden=true href=#backpropagation>#</a></h3><p>Backpropagation is the algorithm used to train neural networks by computing the gradient of the cost function with respect to each weight in the network. It involves two phases:</p><ul><li><strong>Forward Pass:</strong> The input data is passed through the network to calculate the output and the cost.</li><li><strong>Backward Pass:</strong> The gradient of the cost function is calculated using the chain rule of calculus, and the weights are updated accordingly to minimize the cost. This process is repeated for multiple iterations (epochs) until the model converges.</li></ul><h3 id=neurons-count-in-neural-network><strong>Neurons Count in Neural Network</strong><a hidden class=anchor aria-hidden=true href=#neurons-count-in-neural-network>#</a></h3><p>The number of neurons in each layer of a neural network determines the network&rsquo;s capacity to learn from data. More neurons allow the network to capture more complex features, but too many neurons can lead to overfitting. The optimal number of neurons depends on the complexity of the task and the amount of training data available.</p><h3 id=layers-count-in-neural-network><strong>Layers Count in Neural Network</strong><a hidden class=anchor aria-hidden=true href=#layers-count-in-neural-network>#</a></h3><p>The number of layers in a neural network, particularly the number of hidden layers, determines the depth of the network. A deeper network with more layers can learn more abstract features and represent more complex relationships in the data. However, deeper networks are also more computationally expensive to train and can suffer from issues like vanishing gradients. The right number of layers depends on the specific problem and the data.</p><h3 id=steps-to-create-a-neural-network><strong>Steps to Create a Neural Network</strong><a hidden class=anchor aria-hidden=true href=#steps-to-create-a-neural-network>#</a></h3><ol><li><p><strong>Choose Neurons and Layers Count and Biases:</strong></p><ul><li>Select the number of neurons in each layer and the number of layers based on the problem&rsquo;s complexity.</li><li>Initialize weights and biases, usually with small random values.</li></ul></li><li><p><strong>Training:</strong></p><ul><li>Feed the training data through the network, performing forward and backward passes to update the weights.</li></ul></li><li><p><strong>Apply Cost Function:</strong></p><ul><li>Calculate the error between the predicted output and the actual output using the cost function.</li></ul></li><li><p><strong>Apply Backpropagation:</strong></p><ul><li>Perform backpropagation to calculate gradients and update weights.</li></ul></li><li><p><strong>Adjust Weights:</strong></p><ul><li>Iterate through the training process multiple times (epochs) to adjust the weights and minimize the cost function.</li></ul></li></ol><h2 id=6-evaluating-model-performance><strong>6. Evaluating Model Performance</strong><a hidden class=anchor aria-hidden=true href=#6-evaluating-model-performance>#</a></h2><h3 id=evaluate-hypothesis><strong>Evaluate Hypothesis</strong><a hidden class=anchor aria-hidden=true href=#evaluate-hypothesis>#</a></h3><p>Evaluating a hypothesis in the context of machine learning involves assessing how well a model (or hypothesis) performs in making predictions on new, unseen data. The hypothesis refers to the model&rsquo;s assumptions about the underlying data distribution and its ability to generalize from the training data to the test data. Evaluation metrics such as accuracy, precision, recall, F1-score, and the confusion matrix are commonly used to determine the effectiveness of the model. The goal is to ensure that the model&rsquo;s predictions are accurate and reliable, minimizing errors on both the training and test sets.</p><h3 id=overfitting><strong>Overfitting</strong><a hidden class=anchor aria-hidden=true href=#overfitting>#</a></h3><p>Overfitting occurs when a machine learning model performs exceptionally well on the training data but fails to generalize to new, unseen data. This happens when the model learns not only the underlying patterns in the data but also the noise and outliers. As a result, the model becomes overly complex and performs poorly on the test set. Overfitting can be identified when there&rsquo;s a large discrepancy between the training error (low) and the test error (high). Techniques like regularization, cross-validation, and simplifying the model can help prevent overfitting.</p><h3 id=training-error><strong>Training Error</strong><a hidden class=anchor aria-hidden=true href=#training-error>#</a></h3><p>Training error is the error (or loss) calculated on the training dataset, which is the data the model was trained on. It measures how well the model fits the training data. A low training error indicates that the model has learned the patterns in the training data well. However, a very low training error, especially if the test error is high, might indicate that the model has overfitted the training data, capturing even the noise instead of just the underlying trends.</p><h3 id=cross-validation-error><strong>Cross-Validation Error</strong><a hidden class=anchor aria-hidden=true href=#cross-validation-error>#</a></h3><p>Cross-validation error is the error calculated during the cross-validation process, where the training dataset is split into multiple subsets (folds), and the model is trained and validated on different combinations of these subsets. The most common technique is k-fold cross-validation, where the data is divided into k subsets, and the model is trained k times, each time leaving out one of the subsets for validation. The cross-validation error provides a more reliable estimate of the model&rsquo;s performance on unseen data compared to just using the training error. It helps in tuning hyperparameters and selecting the best model.</p><h3 id=test-error><strong>Test Error</strong><a hidden class=anchor aria-hidden=true href=#test-error>#</a></h3><p>Test error is the error calculated on the test dataset, which is a separate portion of the data not used during the training process. The test error gives an unbiased estimate of how well the model is likely to perform on new, unseen data. A low test error indicates that the model generalizes well, while a high test error might indicate problems like overfitting or underfitting. The goal is to minimize the test error, ensuring that the model&rsquo;s predictions are accurate on real-world data.</p><h2 id=7-bias-and-variance-tradeoff><strong>7. Bias and Variance Tradeoff</strong><a hidden class=anchor aria-hidden=true href=#7-bias-and-variance-tradeoff>#</a></h2><h3 id=underfitting-bias><strong>Underfitting: Bias</strong><a hidden class=anchor aria-hidden=true href=#underfitting-bias>#</a></h3><p>Underfitting occurs when a model is too simple to capture the underlying patterns in the data, leading to poor performance on both the training and test sets. This usually happens when the model has high bias, meaning it makes strong assumptions about the data that aren&rsquo;t accurate. High bias models tend to miss the relevant relationships between features and the target variable. Techniques to reduce underfitting include increasing the model complexity (e.g., adding more features, using a more complex model), or reducing regularization.</p><h3 id=overfitting-variance><strong>Overfitting: Variance</strong><a hidden class=anchor aria-hidden=true href=#overfitting-variance>#</a></h3><p>Overfitting, as mentioned earlier, happens when a model is too complex and captures the noise in the training data along with the underlying patterns. This leads to high variance, meaning the model is highly sensitive to the specific training data and may perform poorly on new data. High variance models have low training error but high test error. Techniques like cross-validation, regularization, and pruning (in decision trees) can help reduce overfitting and control variance.</p><h2 id=8-regular-expressions-regex-in-nlp><strong>8. Regular Expressions (Regex) in NLP</strong><a hidden class=anchor aria-hidden=true href=#8-regular-expressions-regex-in-nlp>#</a></h2><h3 id=regex><strong>Regex</strong><a hidden class=anchor aria-hidden=true href=#regex>#</a></h3><p>A Regular Expression (Regex) is a sequence of characters that define a search pattern. In computer science, regex is used for pattern matching within strings. It is a powerful tool for text processing tasks like searching, extracting, and replacing text. For example, the regex pattern <code>\d+</code> matches one or more digits in a text. Regex is widely used in programming for tasks such as validating inputs, parsing text, and transforming data.</p><h3 id=regex-usage-in-nlp><strong>Regex Usage in NLP</strong><a hidden class=anchor aria-hidden=true href=#regex-usage-in-nlp>#</a></h3><p>In Natural Language Processing (NLP), regex is frequently used for text preprocessing tasks such as tokenization, removing unwanted characters (like punctuation or special symbols), and extracting specific patterns (like dates, emails, or phone numbers) from text. Regex can also be used to identify and extract linguistic patterns, such as matching specific word sequences or sentence structures. Its flexibility makes it a valuable tool for cleaning and preparing text data before applying more complex NLP algorithms.</p><h3 id=common-regexes><strong>Common Regexes</strong><a hidden class=anchor aria-hidden=true href=#common-regexes>#</a></h3><p>Common regular expressions include:</p><ul><li><strong><code>\d+</code></strong>: Matches one or more digits.</li><li><strong><code>\w+</code></strong>: Matches one or more word characters (letters, digits, and underscores).</li><li><strong><code>\s+</code></strong>: Matches one or more whitespace characters.</li><li><strong><code>[A-Za-z]+</code></strong>: Matches one or more uppercase or lowercase letters.</li><li><strong><code>^</code> and <code>$</code></strong>: Match the start and end of a string, respectively.</li><li><strong><code>.</code></strong>: Matches any single character except a newline.</li></ul><p>These patterns can be combined and modified using quantifiers, character classes, and anchors to create complex expressions tailored to specific tasks.</p><h2 id=9-nlp-steps><strong>9. NLP Steps</strong><a hidden class=anchor aria-hidden=true href=#9-nlp-steps>#</a></h2><h3 id=tokenizing><strong>Tokenizing</strong><a hidden class=anchor aria-hidden=true href=#tokenizing>#</a></h3><p>Tokenization is the process of breaking down text into smaller units called tokens. In NLP, tokens typically represent words, phrases, or sentences. Tokenization is the first step in many NLP tasks, as it converts unstructured text into a structured format that algorithms can process. For example, the sentence &ldquo;Hello, world!&rdquo; can be tokenized into [&ldquo;Hello&rdquo;, &ldquo;,&rdquo;, &ldquo;world&rdquo;, &ldquo;!&rdquo;]. Tokenization can be done at different levels, such as word-level, sentence-level, or even character-level, depending on the task.</p><h3 id=normalizing-word-format><strong>Normalizing Word Format</strong><a hidden class=anchor aria-hidden=true href=#normalizing-word-format>#</a></h3><p>Word normalization is the process of transforming words into a standard format, which helps reduce the complexity of text data and improve the performance of NLP models. Common normalization techniques include:</p><ul><li><strong>Lowercasing</strong>: Converting all words to lowercase to ensure that words like &ldquo;Apple&rdquo; and &ldquo;apple&rdquo; are treated as the same word.</li><li><strong>Stemming</strong>: Reducing words to their base or root form (e.g., &ldquo;running&rdquo; becomes &ldquo;run&rdquo;).</li><li><strong>Lemmatization</strong>: Converting words to their base form (lemma) based on the word&rsquo;s meaning and context (e.g., &ldquo;better&rdquo; becomes &ldquo;good&rdquo;).</li></ul><p>Normalization helps in reducing variations and inconsistencies in text data, leading to more accurate analysis and modeling.</p><h3 id=segmenting-sentences-in-running-text><strong>Segmenting Sentences in Running Text</strong><a hidden class=anchor aria-hidden=true href=#segmenting-sentences-in-running-text>#</a></h3><p>Sentence segmentation is the process of dividing a running text into individual sentences. This step is crucial in NLP because many tasks, such as sentiment analysis, summarization, and translation, require sentence-level processing. Sentence segmentation typically involves identifying sentence boundaries using punctuation marks like periods, exclamation points, and question marks. However, it can be challenging due to variations in punctuation usage and the presence of abbreviations. Advanced sentence segmentation algorithms use both rule-based and machine learning approaches to accurately detect sentence boundaries.</p><h2 id=10-text-corpora-in-nlp><strong>10. Text Corpora in NLP</strong><a hidden class=anchor aria-hidden=true href=#10-text-corpora-in-nlp>#</a></h2><h3 id=what-is-a-text-corpus><strong>What is a Text Corpus?</strong><a hidden class=anchor aria-hidden=true href=#what-is-a-text-corpus>#</a></h3><p>A text corpus (plural: corpora) is a large and structured set of texts used for linguistic research and NLP tasks. Corpora are used to train and evaluate NLP models, providing a rich source of language data. They can contain text from various domains, such as news articles, books, academic papers, or social media posts. Some corpora are annotated with linguistic information, such as part-of-speech tags, syntactic structures, or semantic roles, which makes them valuable for supervised learning tasks in NLP.</p><h3 id=what-is-the-brown-corpus-in-python><strong>What is the Brown Corpus in Python?</strong><a hidden class=anchor aria-hidden=true href=#what-is-the-brown-corpus-in-python>#</a></h3><p>The Brown Corpus is one of the earliest and most well-known text corpora in the field of computational linguistics. It consists of 1.15 million words from a wide range of genres, including fiction, news, and academic writing, all collected from American English texts published in 1961. The Brown Corpus is fully annotated with part-of-speech tags, making it a valuable resource for training and evaluating NLP models. In Python, the Brown Corpus is available through the Natural Language Toolkit (NLTK) library, where it can be accessed and used for various NLP tasks, such as training taggers or studying language patterns.</p><h3 id=what-is-the-switchboard-corpus><strong>What is the Switchboard Corpus?</strong><a hidden class=anchor aria-hidden=true href=#what-is-the-switchboard-corpus>#</a></h3><p>The Switchboard Corpus is a collection of approximately 2,400 telephone conversations between 543 American English speakers, covering a wide range of topics. The conversations were collected in the early 1990s and have been transcribed and annotated with various linguistic features, including part-of-speech tags, syntactic structures, and discourse markers. The Switchboard Corpus is widely used in NLP research, particularly in the fields of speech recognition, dialog systems, and discourse analysis. It provides a rich dataset for studying conversational speech and language use in informal settings.</p><h2 id=11-text-classification><strong>11. Text Classification</strong><a hidden class=anchor aria-hidden=true href=#11-text-classification>#</a></h2><h3 id=text-classification><strong>Text Classification</strong><a hidden class=anchor aria-hidden=true href=#text-classification>#</a></h3><p>Text classification is the process of assigning predefined categories or labels to a given text. It is a common task in Natural Language Processing (NLP) and can be applied to various applications, such as spam detection, sentiment analysis, topic labeling, and document classification. Text classification involves transforming raw text into a format that machine learning models can process, extracting relevant features, and training a model to recognize patterns associated with different classes. Common algorithms used for text classification include Naive Bayes, Support Vector Machines (SVM), and neural networks.</p><h2 id=12-naive-bayes-in-text-classification><strong>12. Naive Bayes in Text Classification</strong><a hidden class=anchor aria-hidden=true href=#12-naive-bayes-in-text-classification>#</a></h2><h3 id=naive-bayes-in-text-classification><strong>Naive Bayes in Text Classification</strong><a hidden class=anchor aria-hidden=true href=#naive-bayes-in-text-classification>#</a></h3><p>Naive Bayes is a simple yet effective probabilistic algorithm used for text classification. It is based on Bayes&rsquo; theorem and assumes that the features (e.g., words in a text) are independent given the class label. This &ldquo;naive&rdquo; independence assumption makes the model computationally efficient and easy to implement, though it may not always hold true in practice. Despite this, Naive Bayes often performs well in text classification tasks, especially when dealing with large datasets. It is commonly used for tasks like spam detection and sentiment analysis.</p><h2 id=13-bag-of-words-in-naive-bayes><strong>13. Bag of Words in Naive Bayes</strong><a hidden class=anchor aria-hidden=true href=#13-bag-of-words-in-naive-bayes>#</a></h2><h3 id=bag-of-words-in-naive-bayes><strong>Bag of Words in Naive Bayes</strong><a hidden class=anchor aria-hidden=true href=#bag-of-words-in-naive-bayes>#</a></h3><p>The Bag of Words (BoW) model is a feature extraction technique used in text classification and other NLP tasks. It represents text as a collection (or &ldquo;bag&rdquo;) of words, ignoring grammar and word order but keeping track of word frequencies. In the context of Naive Bayes, the BoW model is used to convert text into numerical features that the algorithm can process. Each word in the vocabulary becomes a feature, and its value is typically the word&rsquo;s frequency or presence in a document. This approach simplifies text into a form that can be fed into a machine learning model.</p><h2 id=14-naive-bayes-example><strong>14. Naive Bayes Example</strong><a hidden class=anchor aria-hidden=true href=#14-naive-bayes-example>#</a></h2><h3 id=naive-bayes-example><strong>Naive Bayes Example</strong><a hidden class=anchor aria-hidden=true href=#naive-bayes-example>#</a></h3><p>Consider a simple example of classifying emails as &ldquo;spam&rdquo; or &ldquo;not spam&rdquo; using Naive Bayes. Let&rsquo;s say we have a training dataset with labeled emails. First, we build a vocabulary from the training data and use the Bag of Words model to represent each email as a vector of word frequencies. Then, we calculate the probability of each word occurring in spam and not spam emails using the training data. When a new email arrives, Naive Bayes applies Bayes&rsquo; theorem to calculate the probability that the email belongs to each class and assigns it to the class with the highest probability.</p><h2 id=15-evaluation-metrics-precision-recall-f-measure><strong>15. Evaluation Metrics: Precision, Recall, F-Measure</strong><a hidden class=anchor aria-hidden=true href=#15-evaluation-metrics-precision-recall-f-measure>#</a></h2><h3 id=evaluation-precision-recall-f-measure><strong>Evaluation: Precision, Recall, F-Measure</strong><a hidden class=anchor aria-hidden=true href=#evaluation-precision-recall-f-measure>#</a></h3><p>Evaluation metrics are used to assess the performance of a classification model. The most common metrics include:</p><ul><li><strong>Precision:</strong> The proportion of true positive predictions among all positive predictions. Precision answers the question, &ldquo;Of all the instances the model predicted as positive, how many were actually positive?&rdquo;</li><li><strong>Recall (Sensitivity):</strong> The proportion of true positive predictions among all actual positive instances. Recall answers the question, &ldquo;Of all the actual positive instances, how many did the model correctly identify?&rdquo;</li><li><strong>F-Measure (F1-Score):</strong> The harmonic mean of precision and recall, providing a single metric that balances both. It is useful when the classes are imbalanced.</li></ul><h2 id=16-confusion-matrix><strong>16. Confusion Matrix</strong><a hidden class=anchor aria-hidden=true href=#16-confusion-matrix>#</a></h2><h3 id=confusion-matrix><strong>Confusion Matrix</strong><a hidden class=anchor aria-hidden=true href=#confusion-matrix>#</a></h3><p>A confusion matrix is a table used to describe the performance of a classification model. It provides a breakdown of the model&rsquo;s predictions into four categories:</p><ul><li><strong>True Positives (TP):</strong> Correctly predicted positive instances.</li><li><strong>True Negatives (TN):</strong> Correctly predicted negative instances.</li><li><strong>False Positives (FP):</strong> Incorrectly predicted positive instances (Type I error).</li><li><strong>False Negatives (FN):</strong> Incorrectly predicted negative instances (Type II error).
The confusion matrix is a powerful tool for visualizing the performance of a model and calculating various metrics like accuracy, precision, recall, and F1-score.</li></ul><h2 id=17-formulas-for-precision-recall-accuracy-f-measure><strong>17. Formulas for Precision, Recall, Accuracy, F-Measure</strong><a hidden class=anchor aria-hidden=true href=#17-formulas-for-precision-recall-accuracy-f-measure>#</a></h2><h3 id=formulas-for-precision-recall-accuracy-f-measure><strong>Formulas for Precision, Recall, Accuracy, F-Measure</strong><a hidden class=anchor aria-hidden=true href=#formulas-for-precision-recall-accuracy-f-measure>#</a></h3><p>The formulas for the key evaluation metrics are as follows:</p><ul><li><strong>Precision:</strong> ( \text{Precision} = \frac{TP}{TP + FP} )</li><li><strong>Recall (Sensitivity):</strong> ( \text{Recall} = \frac{TP}{TP + FN} )</li><li><strong>Accuracy:</strong> ( \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} )</li><li><strong>F-Measure (F1-Score):</strong> ( \text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} )</li></ul><p>These metrics provide different perspectives on the model&rsquo;s performance, helping to identify strengths and weaknesses in different aspects of prediction.</p><h2 id=18-logistic-regression-for-text-sentiment-analysis><strong>18. Logistic Regression for Text Sentiment Analysis</strong><a hidden class=anchor aria-hidden=true href=#18-logistic-regression-for-text-sentiment-analysis>#</a></h2><h3 id=how-to-use-logistic-regression-for-text-sentiment-analysis><strong>How to Use Logistic Regression for Text Sentiment Analysis</strong><a hidden class=anchor aria-hidden=true href=#how-to-use-logistic-regression-for-text-sentiment-analysis>#</a></h3><p>Logistic Regression is a linear model used for binary classification, making it suitable for sentiment analysis tasks where the goal is to classify text as having positive or negative sentiment. In this context, the steps include:</p><ol><li><strong>Feature Extraction:</strong> Convert text into numerical features using techniques like Bag of Words or TF-IDF.</li><li><strong>Model Training:</strong> Train the Logistic Regression model on labeled sentiment data, where each text is labeled as positive or negative.</li><li><strong>Prediction:</strong> Apply the trained model to new, unseen text to predict the sentiment.
Logistic Regression outputs a probability that the text belongs to a certain class, with a threshold used to make the final classification decision.</li></ol><h2 id=19-language-models><strong>19. Language Models</strong><a hidden class=anchor aria-hidden=true href=#19-language-models>#</a></h2><h3 id=language-models><strong>Language Models</strong><a hidden class=anchor aria-hidden=true href=#language-models>#</a></h3><p>A language model is a probabilistic model that assigns a probability to a sequence of words in a language. It predicts the likelihood of a word given the previous words in the sequence. Language models are essential in NLP tasks such as speech recognition, machine translation, and text generation. There are different types of language models, including n-gram models, neural language models, and transformer-based models like GPT.</p><h2 id=20-unigram-model><strong>20. Unigram Model</strong><a hidden class=anchor aria-hidden=true href=#20-unigram-model>#</a></h2><h3 id=unigram-model><strong>Unigram Model</strong><a hidden class=anchor aria-hidden=true href=#unigram-model>#</a></h3><p>A unigram model is the simplest type of language model that assumes each word in a sequence is independent of the others. It calculates the probability of a word based on its frequency in the training corpus. For example, the probability of a sentence is the product of the probabilities of each word occurring independently. While simplistic, unigram models are useful for tasks like document classification and word prediction when combined with other models.</p><h2 id=21-bigram-model><strong>21. Bigram Model</strong><a hidden class=anchor aria-hidden=true href=#21-bigram-model>#</a></h2><h3 id=bigram-model><strong>Bigram Model</strong><a hidden class=anchor aria-hidden=true href=#bigram-model>#</a></h3><p>A bigram model is a type of language model that considers the probability of a word based on the preceding word in the sequence. It captures word dependencies and calculates the probability of a word given the previous word. For example, the probability of the word &ldquo;morning&rdquo; following the word &ldquo;good&rdquo; would be calculated based on their co-occurrence in the training corpus. Bigram models are more accurate than unigram models but still have limitations in capturing longer dependencies.</p><h2 id=22-n-gram-model><strong>22. N-Gram Model</strong><a hidden class=anchor aria-hidden=true href=#22-n-gram-model>#</a></h2><h3 id=n-gram-model><strong>N-Gram Model</strong><a hidden class=anchor aria-hidden=true href=#n-gram-model>#</a></h3><p>An n-gram model is a generalization of unigram and bigram models that predicts the probability of a word based on the previous (n-1) words. For example, a trigram model (n=3) predicts a word based on the two preceding words. N-gram models capture more context than unigram and bigram models but require larger datasets to estimate probabilities accurately. As n increases, the model becomes more context-aware but also more computationally expensive.</p><h2 id=23-zero-probability-in-bigrams><strong>23. Zero Probability in Bigrams</strong><a hidden class=anchor aria-hidden=true href=#23-zero-probability-in-bigrams>#</a></h2><h3 id=zero-probability-bigrams><strong>Zero Probability Bigrams</strong><a hidden class=anchor aria-hidden=true href=#zero-probability-bigrams>#</a></h3><p>Zero probability in bigrams occurs when a bigram (a pair of words) is not observed in the training corpus, leading the model to assign it a probability of zero. This is problematic because it means the model will reject any sequence containing that bigram, even if it is a plausible or correct sequence. This issue arises because the bigram model relies on observed frequencies to estimate probabilities.</p><h2 id=24-solution-to-zero-probability-bigrams-smoothing><strong>24. Solution to Zero Probability Bigrams (Smoothing)</strong><a hidden class=anchor aria-hidden=true href=#24-solution-to-zero-probability-bigrams-smoothing>#</a></h2><h3 id=solution-to-zero-probability-bigrams-smoothing><strong>Solution to Zero Probability Bigrams (Smoothing)</strong><a hidden class=anchor aria-hidden=true href=#solution-to-zero-probability-bigrams-smoothing>#</a></h3><p>Smoothing techniques are used to address the problem of zero probabilities in language models. Smoothing adjusts the estimated probabilities to account for unseen events (e.g., bigrams that did not appear in the training data). Common smoothing methods include:</p><ul><li><strong>Laplace Smoothing:</strong> Adds a small constant (usually 1) to all counts to ensure that no probability is zero.</li><li><strong>Good-Turing Smoothing:</strong> Adjusts the probabilities of unseen events based on the frequency of events that were seen once.
These techniques help the model generalize better to unseen data and avoid assigning zero probabilities to valid word sequences.</li></ul><h2 id=25-laplace-smoothing><strong>25. Laplace Smoothing</strong><a hidden class=anchor aria-hidden=true href=#25-laplace-smoothing>#</a></h2><h3 id=laplace-smoothing><strong>Laplace Smoothing</strong><a hidden class=anchor aria-hidden=true href=#laplace-smoothing>#</a></h3><p>Laplace Smoothing, also known as add-one smoothing, is a simple technique used to handle zero probabilities in language models. It works by adding a count of 1 to every possible n-gram, ensuring that even n-grams not seen in the training data have a small non-zero probability. The formula for Laplace smoothing in a bigram model is:
[ P(w_n | w_{n-1}) = \frac{\text{count}(w_{n-1}, w_n) + 1}{\text{count}(w_{n-1}) + V} ]
where ( V ) is the size of the vocabulary. This technique helps prevent the model from assigning zero probability to unseen word pairs.</p><h2 id=26-backoff-and-interpolation><strong>26. Backoff and Interpolation</strong><a hidden class=anchor aria-hidden=true href=#26-backoff-and-interpolation>#</a></h2><h3 id=backoff-and-interpolation><strong>Backoff and Interpolation</strong><a hidden class=anchor aria-hidden=true href=#backoff-and-interpolation>#</a></h3><p>Backoff and interpolation are techniques used in n-gram models to handle cases where higher-order n-grams (e.g., trigrams) have zero counts or low confidence.</p><ul><li><strong>Backoff:</strong> The model &ldquo;backs off&rdquo; to a lower-order n-gram (e.g., bigram) when the higher-order n-gram has a zero count. This ensures that the model can still make a prediction even when some word combinations are unseen.</li><li><strong>Interpolation:</strong> The model combines the probabilities from higher-order and lower-order n-grams by weighting them, rather than backing off completely. The weights are usually determined based on the confidence in the higher-order n-gram counts.
These techniques improve the robustness of language models by ensuring that probabilities are more reliable even with sparse data.</li></ul><h2 id=27-linear-interpolation><strong>27. Linear Interpolation</strong><a hidden class=anchor aria-hidden=true href=#27-linear-interpolation>#</a></h2><h3 id=linear-interpolation><strong>Linear Interpolation</strong><a hidden class=anchor aria-hidden=true href=#linear-interpolation>#</a></h3><p>Linear interpolation is a method used in n-gram language models to combine the probabilities of different n-gram levels (e.g., unigram, bigram, trigram) into a single probability estimate. The formula for linear interpolation in a trigram model might look like:
[ P(w_n | w_{n-1}, w_{n-2}) = \lambda_3 P(w_n | w_{n-1}, w_{n-2}) + \lambda_2 P(w_n | w_{n-1}) + \lambda_1 P(w_n) ]
where ( \lambda_1 + \lambda_2 + \lambda_3 = 1 ) are the interpolation weights. This method helps the model balance the use of context from different levels of n-grams, making it more flexible and accurate.</p><h2 id=28-embedding-methods-tf-idf-and-word2vec><strong>28. Embedding Methods (TF-IDF and Word2Vec)</strong><a hidden class=anchor aria-hidden=true href=#28-embedding-methods-tf-idf-and-word2vec>#</a></h2><h3 id=embedding-methods-tf-idf-and-word2vec><strong>Embedding Methods (TF-IDF and Word2Vec)</strong><a hidden class=anchor aria-hidden=true href=#embedding-methods-tf-idf-and-word2vec>#</a></h3><p>Embedding methods are techniques used to represent words or documents as vectors in a continuous space. These methods capture the semantic meaning of words and their relationships.</p><ul><li><strong>TF-IDF (Term Frequency-Inverse Document Frequency):</strong> A statistical measure used to evaluate the importance of a word in a document relative to a corpus. It combines the frequency of a word in a document (TF) with the inverse frequency of the word across all documents (IDF). TF-IDF is useful for tasks like information retrieval and text classification.</li><li><strong>Word2Vec:</strong> A neural network-based model that learns dense vector representations (embeddings) of words based on their context in a large corpus. Word2Vec captures semantic relationships between words, allowing similar words to have similar vectors. It is widely used in NLP tasks like word similarity, analogy reasoning, and text classification.</li></ul><h2 id=29-term-to-document-matrix><strong>29. Term-to-Document Matrix</strong><a hidden class=anchor aria-hidden=true href=#29-term-to-document-matrix>#</a></h2><h3 id=term-to-document-matrix><strong>Term-to-Document Matrix</strong><a hidden class=anchor aria-hidden=true href=#term-to-document-matrix>#</a></h3><p>A term-to-document matrix is a mathematical representation of a corpus where rows represent terms (words) and columns represent documents. Each cell in the matrix contains a value representing the frequency or weight of the term in the document. This matrix is used in text mining and information retrieval tasks, where it forms the basis for techniques like TF-IDF and Latent Semantic Analysis (LSA). The matrix can be sparse, especially for large corpora, and is often processed using dimensionality reduction techniques to make it more manageable.</p><h2 id=30-word-to-word-matrix><strong>30. Word-to-Word Matrix</strong><a hidden class=anchor aria-hidden=true href=#30-word-to-word-matrix>#</a></h2><h3 id=word-to-word-matrix><strong>Word-to-Word Matrix</strong><a hidden class=anchor aria-hidden=true href=#word-to-word-matrix>#</a></h3><p>A word-to-word matrix is a matrix where both rows and columns represent words, and each cell indicates the relationship or co-occurrence between the two words. This matrix is used in various NLP tasks to capture word associations and similarities. For example, in a co-occurrence matrix, the value in a cell might represent how often two words appear together in a corpus. Word-to-word matrices are used in models like Word2Vec and GloVe to learn word embeddings that capture semantic relationships between words.</p><h2 id=31-tf-idf-formulas><strong>31. TF-IDF Formulas</strong><a hidden class=anchor aria-hidden=true href=#31-tf-idf-formulas>#</a></h2><h3 id=tf-idf-formulas><strong>TF-IDF Formulas</strong><a hidden class=anchor aria-hidden=true href=#tf-idf-formulas>#</a></h3><p>The TF-IDF value for a term ( t ) in a document ( d ) is calculated using the following formulas:</p><ul><li><strong>Term Frequency (TF):</strong> Measures how frequently a term appears in a document.
[ \text{TF}(t, d) = \frac{\text{count}(t, d)}{\text{total terms in } d} ]</li><li><strong>Inverse Document Frequency (IDF):</strong> Measures how important a term is across all documents in the corpus.
[ \text{IDF}(t) = \log\left(\frac{\text{total documents}}{\text{documents containing } t}\right) ]</li><li><strong>TF-IDF:</strong> The product of TF and IDF, giving a weight that reflects the term&rsquo;s importance in the document relative to the corpus.
[ \text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t) ]
TF-IDF is commonly used to convert text into numerical features for machine learning models.</li></ul><h2 id=32-pointwise-mutual-information><strong>32. Pointwise Mutual Information</strong><a hidden class=anchor aria-hidden=true href=#32-pointwise-mutual-information>#</a></h2><h3 id=pointwise-mutual-information><strong>Pointwise Mutual Information</strong><a hidden class=anchor aria-hidden=true href=#pointwise-mutual-information>#</a></h3><p>Pointwise Mutual Information (PMI) is a measure of association between two words or events. It quantifies how much more likely two words are to appear together than if they were independent. The formula for PMI between two words ( w_1 ) and ( w_2 ) is:
[ \text{PMI}(w_1, w_2) = \log\left(\frac{P(w_1, w_2)}{P(w_1) \times P(w_2)}\right) ]
where ( P(w_1, w_2) ) is the joint probability of the two words, and ( P(w_1) ) and ( P(w_2) ) are their individual probabilities. PMI is often used in NLP to identify word associations and semantic similarity.</p><h2 id=33-tf-idf-cons-too-many-dimensions-for-each-vector-and-more><strong>33. TF-IDF Cons (Too Many Dimensions for Each Vector and More)</strong><a hidden class=anchor aria-hidden=true href=#33-tf-idf-cons-too-many-dimensions-for-each-vector-and-more>#</a></h2><h3 id=tf-idf-cons-too-many-dimensions-for-each-vector-and-more><strong>TF-IDF Cons (Too Many Dimensions for Each Vector and More)</strong><a hidden class=anchor aria-hidden=true href=#tf-idf-cons-too-many-dimensions-for-each-vector-and-more>#</a></h3><p>While TF-IDF is a powerful tool for text representation, it has several limitations:</p><ul><li><strong>High Dimensionality:</strong> The number of features (terms) in the TF-IDF matrix can be very large, leading to a high-dimensional space that is computationally expensive to process.</li><li><strong>Sparsity:</strong> The TF-IDF matrix is often sparse, meaning many cells contain zeros. This sparsity can make it challenging to perform efficient computations and may require dimensionality reduction techniques.</li><li><strong>Lack of Semantic Understanding:</strong> TF-IDF does not capture the semantic meaning of words or their relationships, treating each term as independent of others. This limitation can lead to suboptimal performance in tasks that require understanding of word meaning.</li></ul><h2 id=34-word2vec><strong>34. Word2Vec</strong><a hidden class=anchor aria-hidden=true href=#34-word2vec>#</a></h2><h3 id=word2vec><strong>Word2Vec</strong><a hidden class=anchor aria-hidden=true href=#word2vec>#</a></h3><p>Word2Vec is a neural network-based model that learns continuous vector representations of words by analyzing large text corpora. The vectors, also known as word embeddings, capture semantic relationships between words, allowing similar words to have similar vector representations. Word2Vec has two main architectures:</p><ul><li><strong>Skip-Gram:</strong> Predicts the context words given a target word.</li><li><strong>Continuous Bag of Words (CBOW):</strong> Predicts the target word given its context words.
Word2Vec has been widely adopted in NLP tasks such as word similarity, analogy reasoning, and as input to more complex models like neural networks.</li></ul><h2 id=35-hidden-markov-model-hmm><strong>35. Hidden Markov Model (HMM)</strong><a hidden class=anchor aria-hidden=true href=#35-hidden-markov-model-hmm>#</a></h2><h3 id=hidden-markov-model-hmm><strong>Hidden Markov Model (HMM)</strong><a hidden class=anchor aria-hidden=true href=#hidden-markov-model-hmm>#</a></h3><p>A Hidden Markov Model (HMM) is a statistical model that represents systems with hidden (unobservable) states. It is used in various sequential data tasks, including speech recognition, part-of-speech tagging, and bioinformatics. An HMM consists of:</p><ul><li><strong>States:</strong> The hidden states that the system can be in.</li><li><strong>Observations:</strong> The visible outputs generated by the system, which depend on the hidden states.</li><li><strong>Transition Probabilities:</strong> The probabilities of transitioning from one state to another.</li><li><strong>Emission Probabilities:</strong> The probabilities of observing a particular output given a state.
HMMs are used to model processes where the system&rsquo;s true state is not directly observable, but inferences can be made based on the observed data.</li></ul><h2 id=36-transition-probability-matrix-in-hmm><strong>36. Transition Probability Matrix in HMM</strong><a hidden class=anchor aria-hidden=true href=#36-transition-probability-matrix-in-hmm>#</a></h2><h3 id=transition-probability-matrix><strong>Transition Probability Matrix</strong><a hidden class=anchor aria-hidden=true href=#transition-probability-matrix>#</a></h3><p>The transition probability matrix in an HMM defines the probabilities of transitioning from one hidden state to another. Each entry ( a_{ij} ) in the matrix represents the probability of transitioning from state ( i ) to state ( j ):
[ A = [a_{ij}] \text{ where } a_{ij} = P(S_{t+1} = j | S_t = i) ]
This matrix is crucial for determining the likelihood of sequences of states and is used in algorithms like the Viterbi algorithm to decode the most likely sequence of hidden states given a sequence of observations.</p><h2 id=37-states-in-hmm><strong>37. States in HMM</strong><a hidden class=anchor aria-hidden=true href=#37-states-in-hmm>#</a></h2><h3 id=states-in-hmm><strong>States in HMM</strong><a hidden class=anchor aria-hidden=true href=#states-in-hmm>#</a></h3><p>In a Hidden Markov Model, the states represent the hidden (unobservable) conditions or categories that the system can be in at any given time. These states are not directly observable, but they influence the observable outputs or emissions. The sequence of states over time is modeled as a Markov process, where the probability of transitioning to a new state depends only on the current state, not on previous states. The states in an HMM are fundamental to understanding the system&rsquo;s behavior and making predictions based on observed data.</p><h2 id=38-decoding-in-hmm><strong>38. Decoding in HMM</strong><a hidden class=anchor aria-hidden=true href=#38-decoding-in-hmm>#</a></h2><h3 id=decoding-in-hmm><strong>Decoding in HMM</strong><a hidden class=anchor aria-hidden=true href=#decoding-in-hmm>#</a></h3><p>Decoding in HMM refers to the process of determining the most likely sequence of hidden states given a sequence of observations. The most common algorithm used for decoding is the <strong>Viterbi algorithm</strong>, which finds the single most likely sequence of states by dynamically computing the probability of the most likely path to each state at each time step. Decoding is essential for tasks like speech recognition and part-of-speech tagging, where the goal is to infer the underlying state sequence</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://sajadabdollahi.ir/tags/nlp/>Nlp</a></li><li><a href=https://sajadabdollahi.ir/tags/natural-language-processing/>Natural-Language-Processing</a></li><li><a href=https://sajadabdollahi.ir/tags/machine-learning/>Machine-Learning</a></li><li><a href=https://sajadabdollahi.ir/tags/cheatsheet/>Cheatsheet</a></li></ul><nav class=paginav><a class=prev href=https://sajadabdollahi.ir/posts/vscode-plugins/><span class=title>« Prev</span><br><span>3 Necessarily VS Code extensions for a developer</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share NLP basics on x" href="https://x.com/intent/tweet/?text=NLP%20basics&amp;url=https%3a%2f%2fsajadabdollahi.ir%2fposts%2fnlp-brief%2f&amp;hashtags=nlp%2cnatural-language-processing%2cmachine-learning%2ccheatsheet"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share NLP basics on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsajadabdollahi.ir%2fposts%2fnlp-brief%2f&amp;title=NLP%20basics&amp;summary=NLP%20basics&amp;source=https%3a%2f%2fsajadabdollahi.ir%2fposts%2fnlp-brief%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share NLP basics on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsajadabdollahi.ir%2fposts%2fnlp-brief%2f&title=NLP%20basics"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share NLP basics on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsajadabdollahi.ir%2fposts%2fnlp-brief%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share NLP basics on whatsapp" href="https://api.whatsapp.com/send?text=NLP%20basics%20-%20https%3a%2f%2fsajadabdollahi.ir%2fposts%2fnlp-brief%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share NLP basics on telegram" href="https://telegram.me/share/url?text=NLP%20basics&amp;url=https%3a%2f%2fsajadabdollahi.ir%2fposts%2fnlp-brief%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share NLP basics on ycombinator" href="https://news.ycombinator.com/submitlink?t=NLP%20basics&u=https%3a%2f%2fsajadabdollahi.ir%2fposts%2fnlp-brief%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://sajadabdollahi.ir/>Sajad Abdollahi</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>